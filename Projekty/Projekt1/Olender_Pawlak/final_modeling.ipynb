{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding i transformacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_df = pd.read_csv('school_grades_dataset.csv')\n",
    "grades_df = grades_df[grades_df['G3'] != 0]\n",
    "\n",
    "cat_cols = ['school', 'sex', 'address', 'famsize', 'Mjob', 'Fjob', 'reason', 'guardian', 'Pstatus', 'sex', 'school']\n",
    "bin_cols = ['famsup', 'activities', 'nursery', 'internet', 'romantic', 'higher', 'paid', 'schoolsup']\n",
    "\n",
    "grades_df_new = grades_df.drop(columns = (cat_cols + bin_cols))\n",
    "\n",
    "for i in cat_cols:\n",
    "    means = grades_df.groupby(i)['G3'].mean()\n",
    "    grades_df_new[i] = grades_df[i].map(means)\n",
    "    \n",
    "for i in bin_cols:\n",
    "    encoder = ce.OrdinalEncoder(mapping = [{'col': i, 'mapping': {'yes': 1, 'no': 0}},])\n",
    "    grades_df_new[i] = encoder.fit_transform(grades_df)[i]\n",
    "\n",
    "grades_df_new['result'] = pd.cut(grades_df_new['G3'],\n",
    "                                 bins=[-1, 9, 11, 13, 15, 21],\n",
    "                                 labels=['1', '2', '3', '4', '5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# funkca do mierzenia poprawności\n",
    "\n",
    "def simple_models(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    lr = LogisticRegression(random_state=1, max_iter=100)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(f'Logistic regression accuracy: {lr.score(X_test, y_test)}')\n",
    "    selector = RFE(lr, n_features_to_select=7, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    print(f'Logistic regression accuracy: {selector.score(X_test, y_test)}. (po zastosowaniu RFE)')\n",
    "    \n",
    "    tree_model = DecisionTreeClassifier()\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    print(f'Decision Tree accuracy: {tree_model.score(X_test, y_test)}')\n",
    "    selector = RFE(tree_model, n_features_to_select=7, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    print(f'Decision Tree accuracy: {selector.score(X_test, y_test)}. (po zastosowaniu RFE)')\n",
    "    \n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(f'Random Forest accuracy: {rf.score(X_test, y_test)}')\n",
    "    selector = RFE(rf, n_features_to_select=7, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    print(f'Random Forest accuracy: {selector.score(X_test, y_test)}. (po zastosowaniu RFE)')\n",
    "    \n",
    "    \n",
    "    #svc = SVC()\n",
    "    #svc.fit(X_train,y_train)\n",
    "    #print(f'SVC accuracy: {svc.score(X_test, y_test)}')\n",
    "    #selector = RFE(svc, n_features_to_select=7, step=1)\n",
    "    #selector = selector.fit(X_train, y_train)\n",
    "    #print(f'SVC accuracy: {selector.score(X_test, y_test)}. (po zastosowaniu RFE)')\n",
    "    \n",
    "    xgb = xgboost.XGBClassifier(eval_metric = 'merror')\n",
    "    xgb.fit(X_train,y_train)\n",
    "    print(f'XGBoost accuracy: {xgb.score(X_test, y_test)}')\n",
    "    selector = RFE(xgb, n_features_to_select=7, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    print(f'XGBoost accuracy: {selector.score(X_test, y_test)}. (po zastosowaniu RFE)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja konkretnego wyniku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzimy możliwość przewidywania oceny końcowej na dwa sposoby: przewidywanie dokładniej oceny oraz przewidywnanie jej przedziału (kubełki 0-9, 10-11, 12-13, 14-15, 16-21).\n",
    "\n",
    "Użyjemy też różnych sposobówprzewidywania to znaczy będziemy używać G1 i G2, które jest mocno skorelowane z G3 lub też nie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane łącznie z G1 i G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = grades_df_new.drop(['G3', 'result'], axis = 1)\n",
    "y = grades_df_new['G3']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać modele radzą sobie bardzo słabo z odgadnięciem konkretnej liczby punktów zdobytej przez ucznia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane bez G1 i G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = grades_df_new.drop(['G1', 'G2', 'G3', 'result'], axis = 1)\n",
    "y = grades_df_new['G3']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bez tych danych jest w ogóle tragicznie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresja liniowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjmy regresji liniowej do przywidywania wyników na podstawie samych G1 i G2, które są mocno skorelowane z G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n",
    "\n",
    "sns.scatterplot(data = grades_df, x = 'G1', y = 'G3', ax = ax1)\n",
    "sns.scatterplot(data = grades_df, x = 'G2', y = 'G3', ax = ax2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# liniowa zalezcnosc miedzy G1, G2, i G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = grades_df_new[['G1', 'G2']]\n",
    "y = grades_df_new['G3']\n",
    "linear_reg = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "linear_reg.fit(X_train, y_train)\n",
    "y_test_predicted = linear_reg.predict(X_test)\n",
    "\n",
    "print(f'RMSE: {mean_squared_error(y_test, y_test_predicted, squared = False)}')\n",
    "print(f'R-squared: {linear_reg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trzeba pamiętać, że regresja liniowa przewiduje wartości ciągłe, spróbujemy zatem zaokrągliz wynik i sprawdźmy ile odpowiedzi zostało odgadniętych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Odesetek dobrze predykowanych zaokrąglonych wyników:\\\n",
    "      {(linear_reg.predict(X_test).round() == y_test).sum() / len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie jest to zachwycająca odpowiedź, ale lepsza od modeli klasyfikujących."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja przedziały wyniku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane bez G1 i G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = grades_df_new.drop(['G1', 'G2', 'G3', 'result'], axis = 1)\n",
    "y = grades_df_new['result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane z G1 i G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = grades_df_new.drop(['G3', 'result'], axis = 1)\n",
    "y = grades_df_new['result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "C = np.arange(0, 2, 0.2)\n",
    "class_weight = [None, 'balanced']\n",
    "fit_intercept = [True, False]\n",
    "l1_ratio = np.arange(0, 1, 0.1)\n",
    "solver = [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\", \"liblinear\"]\n",
    "\n",
    "lr = LogisticRegression(random_state=1, max_iter=100)\n",
    "\n",
    "param_grid = dict(C = C, class_weight = class_weight, fit_intercept = fit_intercept, l1_ratio = l1_ratio, solver = solver)\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 3, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train) #tutaj lepiej zastosować tylko trainset\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=grid_result.best_estimator_\n",
    "score_max = 0\n",
    "\n",
    "for i in range(30):\n",
    "    selector = RFE(best_model, n_features_to_select=i+1, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    if (selector.score(X_test, y_test) > score_max):\n",
    "        feature_number = i+1\n",
    "        selector_best = selector\n",
    "        score_max = selector.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Wynik dla regresji logistycznej: {selector_best.score(X_test, y_test)}. (po zastosowaniu RFE dla {feature_number} zmiennych)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = selector_best.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f'F1-score: {f1_score(y_test, y_pred, average = \"weighted\")}')\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(f'Precision: {precision_score(y_test, y_pred, average = \"weighted\")}')\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print(f'Recall: {recall_score(y_test, y_pred, average = \"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "criterion = ['gini', 'balanced']\n",
    "class_weight = ['balanced', 'balanced_subsample']\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "\n",
    "param_grid = dict(criterion=criterion, class_weight=class_weight, max_depth=max_depth)\n",
    "grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv = 3, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train) #tutaj lepiej zastosować tylko trainset\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "best_model=grid_result.best_estimator_\n",
    "score_max = 0\n",
    "\n",
    "for i in range(30):\n",
    "    selector = RFE(best_model, n_features_to_select=i+1, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    if (selector.score(X_test, y_test) > score_max):\n",
    "        feature_number = i+1\n",
    "        selector_best = selector\n",
    "        score_max = selector.score(X_test, y_test)\n",
    "        \n",
    "y_pred = selector_best.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f'F1-score: {f1_score(y_test, y_pred, average = \"weighted\")}')\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(f'Precision: {precision_score(y_test, y_pred, average = \"weighted\")}')\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print(f'Recall: {recall_score(y_test, y_pred, average = \"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Wynik dla lasu losowego: {selector_best.score(X_test, y_test)}. (po zastosowaniu RFE dla {feature_number} zmiennych)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
