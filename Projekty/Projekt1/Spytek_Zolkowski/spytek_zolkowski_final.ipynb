{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, RFE, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "import dalex as dx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None, \"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.apispreadsheets.com/api/dataset/school-grades/')\n",
    "data = r.json()\n",
    "df = pd.DataFrame(data['data'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podejście 1. - klasyfikacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dobór zmiennych i ich przetworzenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['data'])\n",
    "\n",
    "df['pass']= np.where(df['G3']<10, 0, 1)\n",
    "\n",
    "### Stworzenie nowych kolumn korzystając z dostępnych danych\n",
    "df['Pedu'] = df['Fedu'] + df['Medu']\n",
    "df[\"genrel\"] = df[\"sex\"]+df[\"romantic\"]\n",
    "df[\"Alc\"] = (df[\"Dalc\"]+df[\"Walc\"]) / 10\n",
    "df[[\"absenc\"]] = np.where(df['absences']<8, 0, 1)\n",
    "fail = pd.DataFrame([(1 if a > 0 else 0) for a in df['failures']], columns=[\"fail\"])\n",
    "df = df.join(fail)\n",
    "\n",
    "### Przeskalowanie kolumn \n",
    "df[[\"Pedu\"]]  = df[[\"Pedu\"]] /df['Pedu'].max()\n",
    "df[[\"studytime\"]]  = df[[\"studytime\"]] /df['studytime'].max()\n",
    "df[[\"age\"]] = df[[\"age\"]] /df['age'].max()\n",
    "df[[\"health\"]] = df[[\"health\"]]/df['health'].max()\n",
    "df[[\"goout\"]] = df[[\"goout\"]]/df['goout'].max()\n",
    "df[[\"freetime\"]]  = df[[\"freetime\"]] /df['freetime'].max()\n",
    "df[[\"Dalc\"]] = df[[\"Dalc\"]]/df['Dalc'].max()\n",
    "df[[\"absences\"]] = df[[\"absences\"]]/df['absences'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"Mjob\", \"higher\", \"genrel\", \"address\", \"reason\", \"school\", 'internet']\n",
    "num_features = [\"Pedu\", \"studytime\", \"goout\", \"age\", \"fail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = num_features + cat_features\n",
    "X = df.drop([\"pass\"], axis=1)[features]\n",
    "y = df[\"pass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess numerical feats:\n",
    "num_transformer = SimpleImputer(strategy=\"constant\")\n",
    "\n",
    "# Preprocessing for categorical features:\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical features:\n",
    "preprocessor = ColumnTransformer(transformers=[(\"num\", num_transformer, num_features),\n",
    "                                               (\"cat\", cat_transformer, cat_features)],\n",
    "                                remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_enh = RandomForestClassifier(n_estimators=10,\n",
    "                               max_features=0.4,\n",
    "                               min_samples_split=2,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=33)\n",
    "\n",
    "model_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', rf_model_enh)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "model_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model_pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podejście 2. - regresja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(drop=\"if_binary\", sparse=False)\n",
    "\n",
    "alldf = enc.fit_transform(df.iloc[:, [0,1,3,4,5,8,9,10,11,15,16,17,18,19,20,21,22]])\n",
    "non_encoded = df.iloc[:, [2,14,29]]\n",
    "scaled = df.iloc[:, [6,7,12,13,23,24,25,26,27,28]]/5 \n",
    "\n",
    "non_encoded.iloc[:,0] = non_encoded.iloc[:,0]/22\n",
    "non_encoded.iloc[:,1] = non_encoded.iloc[:,1]/max(non_encoded.iloc[:,1])\n",
    "non_encoded.iloc[:,2] = non_encoded.iloc[:,2]/max(non_encoded.iloc[:,2])\n",
    "\n",
    "\n",
    "\n",
    "X_all = np.append(alldf, non_encoded, axis=1)\n",
    "X_all = np.append(X_all, scaled, axis=1)\n",
    "\n",
    "encoded_names = enc.get_feature_names(input_features= df.iloc[:, [0,1,3,4,5,8,9,10,11,15,16,17,18,19,20,21,22]].columns)\n",
    "enc_names_list = encoded_names.tolist() + non_encoded.columns.tolist()+ scaled.columns.tolist()\n",
    "enc_names_list\n",
    "\n",
    "X_all = pd.DataFrame(X_all, columns=enc_names_list)\n",
    "y = df[[\"G3\"]]\n",
    "\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dobór zmiennych korzystając z gotowych funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_names(selector):\n",
    "    return np.array(pf.get_feature_names(X_train.columns))[selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podział na zbiór testowy i treningowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X_all, y, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyznaczenie baseline'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = np.mean(y_train)\n",
    "baseline = [mn for i in range(len(y_test))]\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2)\n",
    "X_features = pf.fit_transform(X_train)\n",
    "X_test = pf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### chi selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_selector = SelectKBest(chi2, k=12)\n",
    "chi2_selector.fit_transform(X_features, y_train)\n",
    "feature_names(chi2_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### mi selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_selector = SelectKBest(mutual_info_classif, k=12)\n",
    "mi_selector.fit(X_features, y_train)\n",
    "feature_names(mi_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### rfe selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = LogisticRegression(max_iter=2000)\n",
    "# rfe_selector = RFE(estimator, n_features_to_select=10, step=1)\n",
    "# rfe_selector = rfe_selector.fit(X_features, y_train) \n",
    "# feature_names(rfe_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(num_features):\n",
    "    r = 0.1\n",
    "    l = 0.000000000001\n",
    "    c = (l+r)/2\n",
    "    while True:\n",
    "        model_selector = SelectFromModel(\n",
    "            LogisticRegression(penalty=\"l1\", C=c, solver=\"liblinear\", random_state=42)\n",
    "        )\n",
    "        model_selector.fit_transform(X_features, y_train)\n",
    "        feat = len(feature_names(model_selector))\n",
    "        if feat > num_features:\n",
    "            r = c\n",
    "            c = (r+l)/2\n",
    "        elif feat < num_features:\n",
    "            l = c\n",
    "            c = (l+r)/2\n",
    "        else:\n",
    "            break\n",
    "        print(\"Currently on \", len(feature_names(model_selector)), \" features.\")\n",
    "    print(\"Selected \", len(feature_names(model_selector)), \" features.\")\n",
    "    return(model_selector)\n",
    "            \n",
    "                \n",
    "mod = selection(12)\n",
    "feature_names(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wytrenowanie 4 wybranych modeli na automatycznie przygotowanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chi2 = X_features[:, chi2_selector.get_support()]\n",
    "X_mi = X_features[:, mi_selector.get_support()]\n",
    "# X_rfe = X_features[:, rfe_selector.get_support()]\n",
    "X_rfe = X_features[:, [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False,  True,  True, False, False,False, False, False, False, False,  True, False, False, False,False, False, False,  True, False, False, False, False, False,False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False,  True, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False,  True, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False,  True, False, False, True, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False,  True, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False]]\n",
    "X_msel = X_features[:, mod.get_support()]\n",
    "\n",
    "X_chi2_t = X_test[:, chi2_selector.get_support()]\n",
    "X_mi_t = X_test[:, mi_selector.get_support()]\n",
    "X_rfe_t = X_test[:, [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False,  True,  True, False, False,False, False, False, False, False,  True, False, False, False,False, False, False,  True, False, False, False, False, False,False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False,  True, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False,  True, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False,  True, False, False, True, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False,  True, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False,False, False, False, False, False, False, False, False, False]]\n",
    "# X_rfe_t = X_test[:, rfe_selector.get_support()]\n",
    "X_msel_t = X_test[:, mod.get_support()]\n",
    "\n",
    "X_list = [X_chi2, X_mi, X_rfe, X_msel]\n",
    "X_list_t = [X_chi2_t, X_mi_t, X_rfe_t, X_msel_t]\n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(4):\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    sv = SVR(C=1.5)\n",
    "    rf = RandomForestRegressor(n_estimators=20, max_features=0.5, min_samples_split=3, n_jobs=-1, random_state=0)\n",
    "    gb = GradientBoostingRegressor(learning_rate=0.045, n_estimators=100, criterion='mse', random_state=0)\n",
    "\n",
    "    lr.fit(X_list[i], y_train)\n",
    "    lr_pred = lr.predict(X_list_t[i])\n",
    "    lr_err = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "    \n",
    "    sv.fit(X_list[i], y_train)\n",
    "    sv_pred = sv.predict(X_list_t[i])\n",
    "    sv_err = np.sqrt(mean_squared_error(y_test, sv_pred))\n",
    "    \n",
    "    rf.fit(X_list[i], y_train)\n",
    "    rf_pred = rf.predict(X_list_t[i])\n",
    "    rf_err = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    \n",
    "    gb.fit(X_list[i], y_train)\n",
    "    gb_pred = gb.predict(X_list_t[i])\n",
    "    gb_err = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "    \n",
    "    temp_res = [lr_err, sv_err, rf_err, gb_err]\n",
    "    res.append(temp_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trenowanie modeli na wybranych i przetworzonych przez nas zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['data'])\n",
    "\n",
    "### Stworzenie nowych kolumn korzystając z dostępnych danych\n",
    "df['Pedu'] = df['Fedu'] + df['Medu']\n",
    "df[\"genrel\"] = df[\"sex\"]+df[\"romantic\"]\n",
    "df[\"Alc\"] = (df[\"Dalc\"]+df[\"Walc\"]) / 10\n",
    "df[[\"absenc\"]] = np.where(df['absences']<8, 0, 1)\n",
    "fail = pd.DataFrame([(1 if a > 0 else 0) for a in df['failures']], columns=[\"fail\"])\n",
    "df = df.join(fail)\n",
    "\n",
    "### Przeskalowanie kolumn \n",
    "df[[\"Pedu\"]]  = df[[\"Pedu\"]] /df['Pedu'].max()\n",
    "df[[\"studytime\"]]  = df[[\"studytime\"]] /df['studytime'].max()\n",
    "df[[\"age\"]] = df[[\"age\"]] /df['age'].max()\n",
    "df[[\"health\"]] = df[[\"health\"]]/df['health'].max()\n",
    "df[[\"goout\"]] = df[[\"goout\"]]/df['goout'].max()\n",
    "df[[\"freetime\"]]  = df[[\"freetime\"]] /df['freetime'].max()\n",
    "df[[\"Dalc\"]] = df[[\"Dalc\"]]/df['Dalc'].max()\n",
    "df[[\"absences\"]] = df[[\"absences\"]]/df['absences'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dobór kategorycznych i numerycznych zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"Mjob\", \"higher\", \"genrel\", \"address\", \"reason\", \"school\", 'internet']\n",
    "num_features = [\"Pedu\", \"studytime\", \"goout\", \"age\", \"fail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = num_features + cat_features\n",
    "X = df.drop([\"G3\"], axis=1)[features]\n",
    "y = df[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesssing numerycznych zmiennych:\n",
    "num_transformer = SimpleImputer(strategy=\"constant\")\n",
    "\n",
    "# Preprocesssing kategorycznych zmiennych:\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                                  (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "                                  (\"onehot\", OneHotEncoder(handle_unknown='ignore'))\n",
    "                                 ]\n",
    "                          )\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[(\"num\", num_transformer, num_features),\n",
    "                                               (\"cat\", cat_transformer, cat_features)],\n",
    "                                 remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wytrenowanie wybrancyh modeli z dobranymi ręcznie hiperparametrami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(learning_rate=0.045, n_estimators=100, criterion='mse', random_state=0)\n",
    "rf = RandomForestRegressor(n_estimators=20, max_features=0.5, min_samples_split=3, n_jobs=-1, random_state=0)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "svr = SVR(C=1.5)\n",
    "\n",
    "model_pipe_gb = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', gb)])\n",
    "model_pipe_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', rf)])\n",
    "model_pipe_lr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', lr)])\n",
    "model_pipe_svr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('model', svr)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.1, random_state=42)\n",
    "\n",
    "model_pipe_gb.fit(X_train, y_train)\n",
    "model_pipe_rf.fit(X_train, y_train)\n",
    "model_pipe_lr.fit(X_train, y_train)\n",
    "model_pipe_svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sprawdzenie wyników i porównanie modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_gb = model_pipe_gb.predict(X_test)\n",
    "y_predict_rf = model_pipe_rf.predict(X_test)\n",
    "y_predict_lr = model_pipe_lr.predict(X_test)\n",
    "y_predict_svr = model_pipe_svr.predict(X_test)\n",
    "\n",
    "res.append([\n",
    "    np.sqrt(mean_squared_error(y_test, y_predict_lr)),\n",
    "    np.sqrt(mean_squared_error(y_test, y_predict_svr)),\n",
    "    np.sqrt(mean_squared_error(y_test, y_predict_rf)),\n",
    "    np.sqrt(mean_squared_error(y_test, y_predict_gb))\n",
    "])\n",
    "    \n",
    "#     )], columns=[\"Logistic Regression\", \"SVR\", \"Random Forest\",\"Gradient Boosting\"])\n",
    "    \n",
    "results = pd.DataFrame(res, columns=[\"Logistic Regression\", \"SVR\", \"Random Forest\",\"Gradient Boosting\"], index=[\"SelectKBest (chi2)\",\"SelectKBest (mutual information)\", \"RFE\", \"L1 Based Model Selection\", \"Hand-prepared features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analiza najlepszego modelu - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dx.Explainer(model_pipe_gb,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Zbadanie wpływu zmiennych na predykcję dla dwóch losowych rekordów - przeciętnego i słabego wyniku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.predict_parts(X.loc[[10],:]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.predict_parts(X.loc[[432],:]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
