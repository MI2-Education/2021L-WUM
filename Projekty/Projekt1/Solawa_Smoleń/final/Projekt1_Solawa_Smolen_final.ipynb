{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstęp do Uczenia Maszynowego - Projekt 1\n",
    "## Autorzy: Katarzyna Solawa, Jan Smoleń\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tematem naszego projektu jest przewidywanie przynależności partyjnej członka Izby Reprezentantów amerykańskiego kongresu w 1986 roku na podstawie dokonanych przez niego wyborów podczas głosowań. Naszym zbiorem danych jest ramka zawierająca dane o przynależności partyjnej poszczególnych reprezentantów i ich głosach podczas 16 kluczowych w tym roku głosowań. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import random\n",
    "from sklearn import manifold\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics as metrics\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"congressional_voting_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objaśnienie zmiennych\n",
    "Kolumny 0-15 zawierają wyniki głosowań na tematy skrótowo opisane w nazwach kolumn. Każdy rząd odpowiada jednemu reprezentantowi. Możliwe wartości: <br>\n",
    "**y** - głos na tak <br>\n",
    "**n** - głos na nie <br>\n",
    "**?** - brak głosu - niewzięcie udziału w głosowaniu lub wstrzymanie się od głosu <br>\n",
    "Ostatnia kolumna zawiera informacje o przynależności partyjnej reprezentanta - **republican** albo **democrat**. W naszej ramce danych nie występuje bezpośrednio problem braku danych, ale zapewne będzie trzeba jakoś rozwiązać kwestię wartości **?**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"y\", \"n\", \"?\"]\n",
    "fig, axs = plt.subplots(ncols=2, nrows=8, figsize=(16, 32))\n",
    "for i in range(len(df.columns)-1):\n",
    "    col=df.columns[i]\n",
    "    tmp=df[[col, \"political_party\"]].groupby([\"political_party\", col]).size().tolist()\n",
    "    r, c= i//2, i%2\n",
    "    axs[r,c].bar(labels, list(reversed(tmp[0:3])), label='democrat', color=\"blue\")\n",
    "    axs[r,c].bar(labels, list(reversed(tmp[3:6])), bottom=list(reversed(tmp[0:3])),\n",
    "       label='republican', color=\"red\")\n",
    "    axs[r,c].legend()\n",
    "    axs[r,c].set_title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obie partie głosowały podobnie na `water_project_cost_sharing` oraz `imigration`(lecz u demokratów przeważa `no`, a u republikan `yes`)\n",
    "Widoczna róznica głosów dla:\n",
    "- `adoption_of_the_budget_resolution`(r-no, d-yes)\n",
    "- `physician_fee_freeze`(r-yes, d-no)\n",
    "- `el_salvador_aid`(r-yes, d-no)\n",
    "- `education_spending`(r-yes, d-no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace(\"n\", 0)\n",
    "df=df.replace(\"y\", 1)\n",
    "df=df.replace(\"?\",  0.5)   #rozwiązanie tymczasowe\n",
    "df=df.replace(\"republican\", 0)\n",
    "df=df.replace(\"democrat\",  1)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(), annot=True, annot_kws={'size': 8}, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy, poziom korelacjii pomiędzy głosem a partią bardzo się różni w zależności od tematu głosowania - dla głosowania **water_project_cost_sharing** związek praktycznie nie istnieje, a dla **physician_fee_freeze** jest bardzo duży."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujemy teraz zobaczyć, na ile głosy poszczególnych reprezentantów przypominają głosy innych członków tej samej partii - w tym celu przekształcimy zapisy głosowań poszczególnych członków na wektory i policzymy odległości pomiędzy każdą parą. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adist=sklearn.metrics.pairwise_distances(df.drop([\"political_party\"], axis=1))\n",
    "adist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy teraz funkcji z pakietu manifold żeby przekształcić ramkę zawierającą wzajemne odległości na zbiór współrzędnych na dwuwymiarowej płaszczyźnie. Jest to rzut, który próbuje przekształcić wielowymiarowe zależności na płaszczyznę 2D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"political_party\"]=df[\"political_party\"].replace(0, \"republican\")\n",
    "df[\"political_party\"]=df[\"political_party\"].replace(1, \"democrat\")\n",
    "adist=np.array(adist)\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6)\n",
    "results = mds.fit(adist)\n",
    "coords = results.embedding_\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    coords[:, 0], coords[:, 1], marker = 'o', hue=df[\"political_party\"], palette=[\"red\", \"blue\"]\n",
    "    )\n",
    "ax.set_title(\"Voting pattern similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo sprawdźmy czy któraś z parti ma skołonność do głosowania na tak lub nie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"congressional_voting_dataset.csv\")\n",
    "democrat_df = df[df['political_party'] == 'democrat']\n",
    "republican_df = df[df['political_party'] == 'republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tak = 0\n",
    "nie = 0\n",
    "brak = 0\n",
    "for i in range(0,15):\n",
    "    tak += (democrat_df[democrat_df.columns[i]] == \"y\").sum()\n",
    "    nie += (democrat_df[democrat_df.columns[i]] == \"n\").sum()\n",
    "    brak += (democrat_df[democrat_df.columns[i]] == \"?\").sum()\n",
    "\n",
    "labels = ['Yes', 'No', '?']\n",
    "sizes = [tak, nie,brak]\n",
    "plt.title(\"Democrat\")\n",
    "plt.bar(labels, sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tak = 0\n",
    "nie = 0\n",
    "brak = 0\n",
    "for i in range(0,15):\n",
    "    tak += (republican_df[republican_df.columns[i]] == \"y\").sum()\n",
    "    nie += (republican_df[republican_df.columns[i]] == \"n\").sum()\n",
    "    brak += (republican_df[republican_df.columns[i]] == \"?\").sum()\n",
    "\n",
    "labels = ['Yes', 'No', '?']\n",
    "sizes = [tak, nie,brak]\n",
    "plt.title(\"Republican\")\n",
    "plt.bar(labels, sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W obu przypadkach licza głosów jest dość wyrównana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import random\n",
    "from sklearn import manifold\n",
    "import xgboost as xgb\n",
    "random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from matplotlib import pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"congressional_voting_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "W naszych danych kodowanie zmiennych kategorycznych wydaje się nie być dużym wyzwaniem. Głosy na nie oznaczamy jako 0, brak głosu  jako 0.5, a głosy na tak to 1. Podobnie intuicyjnie republikanów oznaczamy jako zera, a demokratów jako jedynki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace(\"n\", 0)\n",
    "df=df.replace(\"y\", 1)\n",
    "df=df.replace(\"?\",  0.5)  \n",
    "df=df.replace(\"republican\", 0)\n",
    "df=df.replace(\"democrat\",  1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Ze względu na kategoryczne wartości w naszych danych, nie widzimy tu outlierów w postaci rzędów, które się szczególnie wyróżniają jedną wartością. Jedyny rząd, który odrzucimy to ten, w którym wartości wszystkich głosowań wynosiły \"?\" - jest to prawdopodobnie brak danych, bądź dany reprezentant z jakiś osobliwych powodów nie wziął udziału w żadnym głosowaniu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop([\"political_party\"], axis=1)\n",
    "indexes=[]\n",
    "colnames=X.columns\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(colnames)):\n",
    "        if X.iloc[i, j]!=0.5:\n",
    "            break\n",
    "        if j==len(colnames)-1:\n",
    "            indexes.append(i)\n",
    "X=X.drop(248, axis=0)\n",
    "y=df[\"political_party\"].drop(248, axis=0)\n",
    "df=df.drop(248, axis=0)\n",
    "df.to_csv(\"df_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Na początku korzystaliśmy z bardziej intycuicyjnych sposobów wyborów cech, a potem zastosowaliśmy metody pokazane na laboratoriach. Spojrzymy najpierw ponownie na macierz korelacji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(), annot=True, annot_kws={'size': 8}, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuniemy dwie zmienne, które w porównaniu z innymi są bardzo mało skorelowane z naszym celem - **water_project_cost_sharing** i **immigration**. Spróbujemy też usunąć zmienną **el_salvador_aid** - mimo, że jest silnie związana z celem, jest także najbardziej skorelowana z innymi zmiennymi objaśniającymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=[\"water_project_cost_sharing\", \"immigration\", \"el_salvador_aid\"]\n",
    "X=X.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Naszym modelem bazowym, którego dziś użyjemy, będzie xgboost bez tuningu hiperparametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "preds = xgb_model.predict(X_test)\n",
    "comparison = pd.DataFrame({'actual':y_test, 'predicted':preds})\n",
    "print(\"Accuracy: \" + str(sum(comparison[\"actual\"] == comparison[\"predicted\"]) / len(comparison) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy, nasz model całkiem dobrze sobie radzi z przewidywaniem przynależności do danej partii politycznej - osiąga ponad 96% skuteczności. Na koniec spojrzymy, jak ważne dla niego są poszczególne kolumny - użyjemy do tego wbudowanej funkcji modelu xgb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "pyplot.bar(X.columns, xgb_model.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z oczekiwaniami wynikającymi z mapy korelacji, zmienna **physicican_fee_freeze** ma olbrzymi wpływ na predykcje naszego modelu. Na koniec spójrzmy jeszcze, jak wyglądała by skuteczność modelu, gdybyśy wybrali jedynie 5 najważniejszych wg wykresu cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts=[\"physician_fee_freeze\", \"mx_missile\", \"synfuels_corporation_cutback\", \"religious_groups_in_schools\", \"crime\"]\n",
    "X=X[fts]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.2, random_state = 42)\n",
    "xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "preds = xgb_model.predict(X_test)\n",
    "comparison = pd.DataFrame({'actual':y_test, 'predicted':preds})\n",
    "print(\"Accuracy: \" + str(sum(comparison[\"actual\"] == comparison[\"predicted\"]) / len(comparison) * 100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import random\n",
    "from sklearn import manifold\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"congressional_voting_dataset.csv\")\n",
    "df=df.replace(\"n\", 0)\n",
    "df=df.replace(\"y\", 1)\n",
    "df=df.replace(\"?\",  0.5)  \n",
    "df=df.replace(\"republican\", 0)\n",
    "df=df.replace(\"democrat\",  1)\n",
    "X=df.drop([\"political_party\"], axis=1)\n",
    "indexes=[]\n",
    "colnames=X.columns\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(colnames)):\n",
    "        if X.iloc[i, j]!=0.5:\n",
    "            break\n",
    "        if j==len(colnames)-1:\n",
    "            indexes.append(i)\n",
    "X=X.drop(248, axis=0)\n",
    "y=df[\"political_party\"].drop(248, axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=[\"water_project_cost_sharing\", \"immigration\", \"el_salvador_aid\"]\n",
    "X=X.drop(drop, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.2, random_state = 42)\n",
    "X_best=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeature(selector):\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_fs = selector.transform(X_train)\n",
    "    X_test_fs = selector.transform(X_test)\n",
    "    xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)\n",
    "    xgb_model.fit(X_train_fs, y_train)\n",
    "    yhat = xgb_model.predict(X_test_fs)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeature2(selector):\n",
    "    clf = Pipeline([\n",
    "      ('feature_selection', SelectFromModel(selector)),\n",
    "      ('classification', xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0))\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,14):\n",
    "    selectFeature(SelectKBest(chi2, k=i))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najwyższe accuracy dla wyboru wszytkich 13 zmiennych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "estimator = LogisticRegression()\n",
    "for i in range(1,14):\n",
    "    selectFeature(RFE(estimator, n_features_to_select=i, step=1))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najwyższe accuracy przy wyborze min 4 zmiennych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual info classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,14):\n",
    "    selectFeature(SelectKBest(mutual_info_classif, k=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najwyższe accuracy przy wyborze min 10 zmiennych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC - Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C = 0.005\n",
    "selectFeature2(LinearSVC(C=0.005, penalty=\"l1\", dual=False))\n",
    "\n",
    "# C = 0.005\n",
    "selectFeature2(LinearSVC(C=0.01, penalty=\"l1\", dual=False))\n",
    "\n",
    "# C = 0.11 \n",
    "selectFeature2(LinearSVC(C=0.11, penalty=\"l1\", dual=False))\n",
    "\n",
    "# C = 0.17 \n",
    "selectFeature2(LinearSVC(C=0.17, penalty=\"l1\", dual=False))\n",
    "\n",
    "# C = 0.2\n",
    "selectFeature2(LinearSVC(C=0.2, penalty=\"l1\", dual=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectFeature2(LinearSVC())#domyślnie C = 1, penalty='l2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=3)\n",
    "pf.fit(X_train, y_train)\n",
    "X_train_fs = pf.transform(X_train)\n",
    "X_test_fs = pf.transform(X_test)\n",
    "xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)\n",
    "xgb_model.fit(X_train_fs, y_train)\n",
    "yhat = xgb_model.predict(X_test_fs)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeaturePF(selector):\n",
    "    pf = PolynomialFeatures(degree=3)\n",
    "    pf.fit(X_train, y_train)\n",
    "    X_train_pf = pf.transform(X_train)\n",
    "    X_test_pf = pf.transform(X_test)\n",
    "    selector.fit(X_train_pf, y_train)\n",
    "    X_train_fs = selector.transform(X_train_pf)\n",
    "    X_test_fs = selector.transform(X_test_pf)\n",
    "    xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)\n",
    "    xgb_model.fit(X_train_fs, y_train)\n",
    "    yhat = xgb_model.predict(X_test_fs)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    print(f'i = {i}')\n",
    "    selectFeaturePF(SelectKBest(chi2, k=i))   \n",
    "    \n",
    "print(f'i = {42}')\n",
    "selectFeaturePF(SelectKBest(chi2, k=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    print(f'i = {i}')\n",
    "    selectFeaturePF(SelectKBest(mutual_info_classif, k=i))\n",
    "    \n",
    "print(f'i = {37}')\n",
    "selectFeaturePF(SelectKBest(mutual_info_classif, k=37))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy wyborze do k najlepszych zmiennych za pomocą chi2 oraz mutual info classif, accuracy jest większe niż 94.25 dla k min równego 42 i 37."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metody osiągają max accuracy 96.55, czyli tyle ile udało nam sie osiągnąć przy samodzielnym wyborze zmiennych. </br>  \n",
    "Używając niedużej ilości zmiennych udało nam się zatem osiągnąć wynik tylko nieznacznie gorszy od bazowego XGBoosta, który wynosił 97%. Jednak ze względu na małą liczbę rekordów i krótki czas wykonywania algorytmów, nie widzeliśmy sensu w ograniczaniu w tym przypadku liczby kolumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts=[\"physician_fee_freeze\",\"synfuels_corporation_cutback\", \"adoption_of_the_budget_resolution\", \"education_spending\"]\n",
    "X2=X[fts]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, stratify=y, test_size = 0.2, random_state = 42)\n",
    "xgb_model = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "preds = xgb_model.predict(X_test)\n",
    "comparison = pd.DataFrame({'actual':y_test, 'predicted':preds})\n",
    "print(\"Accuracy: \" + str(sum(comparison[\"actual\"] == comparison[\"predicted\"]) / len(comparison) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tmp=xgb.XGBClassifier(objective = \"binary:logistic\", seed = 42, use_label_encoder=False, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wybór modelu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_encoded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chociaż na we wcześniejszym etapie pracy usunęliśmy z naszych danych zduplikowane rzędy, to podczas pracy z modelami okazuje się, że osiągają one lepsze wyniki kiedy je zostawimy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df.drop([\"political_party\"], axis=1)\n",
    "y=df[\"political_party\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybraliśmy do przetestowania 3 modele - **SVM**, **Random Forest** i **XGBoost**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_base=SVC(random_state=42)\n",
    "svm_base.fit(X_train, y_train)\n",
    "preds=svm_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_base_acc=accuracy_score(preds,y_test)\n",
    "print(\"Accuracy SVM z domyślnymi hiperparametrami: \" + str(svm_base_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że już domyślny SVM osiąga bardzo dobre accuracy na poziomie ponad 96%. Spróbujemy teraz wykonać tuning hiperparametrów. Ponieważ nasz zbiór danych jest stosunkowo mały, skorzystamy z narzędzia GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_tuned=SVC(random_state=42)\n",
    "c=[]  # wartości parametru C\n",
    "gamma=[]  #wartości parametru gamma \n",
    "for i in range(-4, 5):      # orientacyjne wartości na podstawie informacji znalezionych w internecie\n",
    "    c.append(10**i)\n",
    "for i in range(-4, 5):\n",
    "    gamma.append(10**i)\n",
    "gamma.append(\"auto\")\n",
    "gamma.append(\"scale\")\n",
    "params = [{'C': c,   \n",
    "           \"kernel\": [\"rbf\", \"linear\", \"poly\"],\n",
    "        'gamma': gamma}]\n",
    "gs_svm=GridSearchCV(svm_tuned, param_grid=params, scoring='accuracy', cv=4, n_jobs=2)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "gs_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tuned_acc=accuracy_score(gs_svm.predict(X_test),y_test)\n",
    "print(\"Accuracy SVM po tuningu hiperparametrów: \" + str(svm_tuned_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy, nie udało nam się polepszyć wyniku, a nawet uzyskaliśmy accuracy trochę gorsze. Mimo że w naszej ramce danych znajdują się także domyślne wartości hiperparametrów, to wypadły one gorzej przy kroswalidacji i dlatego algorytm ich nie wybrał. Wydaje mi się, że taka sytuacja zachodzi ze względu na małą liczbę rekordów i duże accuracy naszych modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_base = xgb.XGBClassifier(objective = \"binary:logistic\", seed = 1613, use_label_encoder=False, verbosity=0)\n",
    "xgb_base.fit(X_train, y_train)\n",
    "preds=xgb_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_base_acc=accuracy_score(preds,y_test)\n",
    "print(\"Accuracy XGB z domyślnymi hiperparametrami: \" + str(xgb_base_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned=xgb.XGBClassifier(objective = \"binary:logistic\", seed = 1613, use_label_encoder=False, eval_metric=\"error\")\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "gs_xgb=GridSearchCV(xgb_tuned, param_grid=params, scoring='accuracy', cv=4, n_jobs=2)\n",
    "gs_xgb.fit(X_train, y_train)\n",
    "gs_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned_acc=accuracy_score(gs_xgb.predict(X_test), y_test)\n",
    "print(\"Accuracy XGB po treningu hiperparametrów: \" + str(xgb_tuned_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_base = RandomForestClassifier(random_state=16)\n",
    "rfc_base.fit(X_train, y_train)\n",
    "preds=rfc_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_base_acc=accuracy_score(preds,y_test)\n",
    "print(\"Accuracy RFC z domyślnymi hiperparametrami: \" + str(rfc_base_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tuned=RandomForestClassifier(random_state=16)\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 1000, num = 5)] # przykładowe wartości znalezione w internecie \n",
    "max_depth = [int(x) for x in np.linspace(5, 55, num = 5)]\n",
    "max_features= ['auto', 'sqrt', 'log2']\n",
    "             \n",
    "params = [{'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "          'max_features': max_features}]\n",
    "gs_rfc=GridSearchCV(rfc_tuned, param_grid=params, scoring='accuracy', cv=4, n_jobs=2)\n",
    "gs_rfc.fit(X_train, y_train)\n",
    "gs_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tuned_acc=accuracy_score(gs_rfc.predict(X_test), y_test)\n",
    "print(\"Accuracy RFC po treningu hiperparametrów: \" + str(rfc_tuned_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, ani w XGBooście, ani w Random Forest nie udało się uzyskać lepszej niż domyślna accuracy. Co więcej, obydwa bazowe modele osiągają dokładnie ten sam wynik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocena modeli\n",
    "## Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "labels=[]\n",
    "scores.append(svm_base_acc)\n",
    "labels.append(\"SVM\")\n",
    "scores.append(xgb_base_acc)\n",
    "labels.append(\"XGB\")\n",
    "scores.append(rfc_base_acc)\n",
    "labels.append(\"RFC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Accuracy Score\": scores}, index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, svm_base.predict(X_test)).ravel()\n",
    "pd.DataFrame({\"Actual positives\": [tp, fp], \"Actual negatives\": [fn, tn]}, index = [\"Positive predictions\", \"Negative predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, xgb_base.predict(X_test)).ravel()\n",
    "pd.DataFrame({\"Actual positives\": [tp, fp], \"Actual negatives\": [fn, tn]}, index = [\"Positive predictions\", \"Negative predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, xgb_base.predict(X_test)).ravel()\n",
    "pd.DataFrame({\"Actual positives\": [tp, fp], \"Actual negatives\": [fn, tn]}, index = [\"Positive predictions\", \"Negative predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svm\n",
    "plt.figure(figsize=(12,10))\n",
    "classifiers = [svm_base, xgb_base, rfc_base]\n",
    "labels=[\"SVM\", \"XGB\", \"RFC\"]\n",
    "ax = plt.gca()\n",
    "for i in range(3):\n",
    "    metrics.plot_roc_curve(classifiers[i], X_test, y_test, ax=ax, name=labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie powyższych wyników zdecydowaliśmy się pozostać przy modelu **XGBoost** bez modyfikacji hiperparametrów czy wyboru zmiennych. W naszym przypadku nawet taki model osiagał bardzo wysokie accuracy, a był przy tym szybki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie \n",
    "## EDA throwback - które predykcje się udały\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp=pd.read_csv(\"congressional_voting_dataset.csv\")\n",
    "df_tmp=df_tmp.replace(\"n\", 0)\n",
    "df_tmp=df_tmp.replace(\"y\", 1)\n",
    "df_tmp=df_tmp.replace(\"?\",  0.5)                                                  #żeby uzyskać taki sam wykres jak w eda\n",
    "df_tmp[\"political_party\"]=df_tmp[\"political_party\"].replace(\"republican\", 0)      #musimy użyć ramki danych z rzędem,\n",
    "df_tmp[\"political_party\"]=df_tmp[\"political_party\"].replace(\"democrat\", 1)        # który wcześniej wyrzucliśmy\n",
    "\n",
    "y_tmp=df_tmp[\"political_party\"]\n",
    "X_tmp=df_tmp.drop([\"political_party\"], axis=1)\n",
    "\n",
    "\n",
    "xgb_tmp=xgb.XGBClassifier(objective = \"binary:logistic\", seed = 1613, use_label_encoder=False, verbosity=0)\n",
    "df_tmp[\"predicted_correctly\"]=False\n",
    "for i in range(len(df_tmp)):\n",
    "    X_train_tmp=X_tmp.drop(i, axis=0)\n",
    "    y_train_tmp=y_tmp.drop(i, axis=0)\n",
    "    X_test_tmp=X_tmp.iloc[[i]]\n",
    "    y_test_tmp=y_tmp[i]\n",
    "    xgb_tmp.fit(X_train_tmp, y_train_tmp)\n",
    "    pred_tmp=xgb_tmp.predict(X_test_tmp)[0]\n",
    "    if y_tmp[i]==pred_tmp:\n",
    "        df_tmp[\"predicted_correctly\"][i]=True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adist=sklearn.metrics.pairwise_distances(df_tmp.drop([\"political_party\", \"predicted_correctly\"], axis=1))\n",
    "adist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp[\"political_party\"]=df_tmp[\"political_party\"].replace(0, \"republican\")\n",
    "df_tmp[\"political_party\"]=df_tmp[\"political_party\"].replace(1, \"democrat\")\n",
    "adist=np.array(adist)\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6)\n",
    "results = mds.fit(adist)\n",
    "coords = results.embedding_\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.scatterplot(\n",
    "    coords[:, 0], coords[:, 1], marker = 'o', hue=df_tmp[\"political_party\"], palette=[\"red\", \"blue\"], \n",
    "    style=df_tmp[\"predicted_correctly\"], markers=[\"X\", \"o\"], size=df_tmp[\"predicted_correctly\"]\n",
    "    )\n",
    "ax.set_title(\"Voting pattern similarity\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na koniec wróciliśmy do wykresu z naszego EDA żeby zobaczyć, których z reprezentantów udało przewidzieć się prawidłowo – wykonaliśmy w tym celu predykcje na podstawie reszty ramki danych dla każdego pojedynczego rzędu. Jak widać, błędne predykcje zazwyczaj znajdują się w zróżnicowanym pod względem partii otoczeniu. Jednak duża skuteczność nawet dla nieoczywistych polityków  pokazuje świadczy o jakości XGBoosta, nawet bez specjalnego dopasowywania cech czy tuningu hiperparametrów. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
