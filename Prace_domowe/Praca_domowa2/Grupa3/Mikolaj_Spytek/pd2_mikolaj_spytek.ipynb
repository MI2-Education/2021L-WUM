{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from category_encoders import TargetEncoder\n",
    "from category_encoders import OneHotEncoder\n",
    "from category_encoders import CountEncoder\n",
    "from category_encoders import OrdinalEncoder\n",
    "import random\n",
    "from math import floor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-netscape",
   "metadata": {},
   "source": [
    "# Praca domowa 2\n",
    "**Mikołaj Spytek**\n",
    "\n",
    "W tej pracy domowej zajmuję się zbiorem danych Allegro. W pierwszej części chodzi o kodowanie zmiennych kategorycznych, natomiast w drugiej o preprocessing - uzupełnianie danych brakujących."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-paraguay",
   "metadata": {},
   "source": [
    "## Część pierwsza\n",
    "\n",
    "W pierwszej części kodujemy zmienne kategoryczne - zmienną `it_location`  za pomocą target encoding oraz zmienną `main_category` za pomocą one-hot encodingu oraz dwóch innych metod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie zbioru danych\n",
    "df = pd.read_csv(\"https://www.dropbox.com/s/360xhh2d9lnaek3/allegro-api-transactions.csv?dl=1\")\n",
    "# zauważyłem, ze nazwy lokalizacji są różnie pisane, więc ujednolicam\n",
    "df[[\"it_location\"]] = df[[\"it_location\"]].apply(lambda x: x.str.lower())\n",
    "\n",
    "#stworzenie kopii, aby każdy typ encodingu był na osobnej ramce\n",
    "dftarget = df.copy()\n",
    "dfonehot = df.copy()\n",
    "dfcount = df.copy()\n",
    "dfordinal = df.copy()\n",
    "df_part2 = df[[\"price\", \"it_seller_rating\", \"it_quantity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"it_location\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-radiation",
   "metadata": {},
   "source": [
    "Gdy spojrzymy na zmienną, którą chcemy zakodować, zauważamy, że przyjmuje ona 7903 różnych wartości. Gdybyśmy więc chieli zastosować one-hot encoding, to powstałoby właśnie tyle nowych kolumn, czyli zmiennych. Ponieważ mamy dużo obserwacji - nadal byłoby około rząd wielkości więcej niż zmiennych, to mogłoby się okazać, że model będzie działać, lecz target encoding jest sposobem na zmniejszenie ilości zmiennych objaśniających."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoding zmiennej it_location\n",
    "en = TargetEncoder()\n",
    "dftarget[\"target_encoded_it_location\"] = en.fit_transform(dftarget[\"it_location\"], dftarget[\"price\"])\n",
    "# sprawdzenie czy pojawiła się nowa kolumna\n",
    "dftarget.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-timber",
   "metadata": {},
   "source": [
    "Widzimy, że w ramce danych pojawiła się nowa kolumna: `target_encoded_it_location`. Jest to zmienna zawierająca zakodowane wartości kategorii.\n",
    "\n",
    "Target encoding polega na pogrupowaniu danych według kategorii, a następnie wyliczenia dla każdej z tych kategorii średniej wartości zmiennej wyjaśnianej. Otrzymaną w ten sposób wartością kodujemy wszystkie obserwacje z danej kategorii.\n",
    "\n",
    "Na przykładzie można sprawdzić, czy ten Encoder rzeczywiście tak działa. Dla Warszawy policzymy średnią cenę produktu \"ręcznie\", a następnie sprawdzimy czy taka sama wartość została nadana przez Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Średnia wyliczona ręcznie: \", dftarget.loc[dftarget[\"it_location\"]==\"warszawa\", \"price\"].mean())\n",
    "print(\"Wartość z encodingu: \", dftarget.loc[dftarget[\"it_location\"]==\"warszawa\", \"target_encoded_it_location\"].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-novel",
   "metadata": {},
   "source": [
    "Wygląda na to, że wszystko się zgadza.\n",
    "\n",
    "#### Jakie są wady takiego kodowania?\n",
    "- zamieniając zmienną kategoryczną na tylko jedną zmienną numeryczną wprowadzamy porządek (możliwość porównania) kategorii, który wcześniej nie istniał, \n",
    "- jeśli w którejś kategorii było mało obserwacji, to średnia wartość targetu może być w łatwy sposób zaburzona, \n",
    "- dane zakodowane w ten sposób mogą powodować przeuczenie się modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"main_category\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-better",
   "metadata": {},
   "source": [
    "Widzimy, że ta zmienna przyjmuje już tylko 27 unikalnych wartości, więc zastosowanie one-hot encodingu ma tu dużo większy sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = OneHotEncoder(use_cat_names=True)\n",
    "\n",
    "dfonehot = dfonehot.join(en.fit_transform(dfonehot[\"main_category\"]))\n",
    "dfonehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-learning",
   "metadata": {},
   "source": [
    "Widzimy, że teraz ramka danych ma 41, kolumny, podczas gdy wcześniej miała 14. Oznacza to, że OneHotEncoder dodał 27 nowych zmiennych - dokładnie tyle ile było unikalnych wartości kolumny `main_category`. Jest to zarówno zaleta, jak i wada tego rodzaju kodowania. Dzięki temu, że każda kategoria ma osobną zmienną, możemy mieć pewność, że nie będą na siebie wpływały w modelu, tak jak to może być w przypadku kodowań, które tworzą tylko jedną zmienną. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = CountEncoder()\n",
    "\n",
    "dfcount[\"count_encoded_main_category\"] = en.fit_transform(dfcount[\"main_category\"])\n",
    "\n",
    "dfcount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-strengthening",
   "metadata": {},
   "source": [
    "W wyniku zastosowania tego kodowania, każdej obserwacji przypisywana jest liczba produktów w danej kategorii. Mamy więc tylko jedną dodatkową kolumnę. W ramach sprawdzenia można popatrzeć na wiersz 2 z powyższej ramki - obserwacja z kategorii `Dom i ogród` została zakodowana jako 91042, a wcześniej gdy sprawdzaliśmy liczbę kategorii, polecenie `df[\"main_category\"].describe()` pokazało, że najczęstszą kategorią jest właśnie ta i pojawia się 91042 razy w tym zbiorze danych.\n",
    "\n",
    "Wadą tego typu encodingu również jest to, że kategoriom nadajemy arbitralny porządek. Dodatkowo kategorie o podobnej liczności będą miały podobną wartość zmiennej kodującej kategorie, a mogą być one zupełnie różny wpływ na zmienną wyjaśnianą.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = OrdinalEncoder()\n",
    "\n",
    "dfordinal[\"ordinal_encoded_main_category\"] = en.fit_transform(dfordinal[\"main_category\"])\n",
    "\n",
    "dfordinal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-equipment",
   "metadata": {},
   "source": [
    "Ta metoda jest bardzo naiwna, i często pociąga niepożądane konsekwencje. Enkoder po prostu przyporządkowuje kategoriom kolejne liczby naturalne. Znów problem pojawia się, ponieważ dostajemy jakiś porządek na kategoriach, którego wcześniej nie było. Model może więc się nauczyć nieistniejących zależności. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-guinea",
   "metadata": {},
   "source": [
    "## Część druga\n",
    "\n",
    "Wypełnianie braków w zmiennych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ustawiamy seed, aby wyniki były powtarzalne\n",
    "random.seed(123)\n",
    "# wybieramy podzbiór kolumn\n",
    "df_part2 = df[[\"price\", \"it_seller_rating\", \"it_quantity\"]]\n",
    "\n",
    "imputer = KNNImputer(weights=\"uniform\", n_neighbors=5)\n",
    "\n",
    "#ograniczmy liczbę rekordów - przy pełnej ramce obliczenia wykonywały się bardzo długo\n",
    "df_part2 = df_part2[:floor(len(df_part2)*0.1)]\n",
    "rmse = [None for i in range(10)]\n",
    "\n",
    "type(df_part2)\n",
    "len(df_part2)\n",
    "for i in range(10):\n",
    "    # robimy kopię ramki\n",
    "    df_part2copy = df_part2.copy()\n",
    "    # wybieramy 10% losowych indeksów\n",
    "    toberemoved = random.sample(range(0, len(df_part2)), floor(len(df_part2)*0.1))\n",
    "    # usuwamy\n",
    "    df_part2copy.iloc[toberemoved, 1] = None\n",
    "    # imputacja\n",
    "    df_part2copy = pd.DataFrame(imputer.fit_transform(df_part2copy), columns=[\"price\", \"it_seller_rating\", \"it_quantity\"])\n",
    "\n",
    "    # wyliczenie błędu\n",
    "    rmse[i] = np.sqrt(mean_squared_error(df_part2[\"it_seller_rating\"], df_part2copy[\"it_seller_rating\"]))\n",
    "    print(\"RMSE {:.2f}\".format(rmse[i]))\n",
    "    \n",
    "    \n",
    "print(\"Odchylenie standardowe miary RMSE w 10 próbach wynosi: {:.2f}\".format(np.std(rmse)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-lighter",
   "metadata": {},
   "source": [
    "Powtarzamy ten sam eksperyment tylko, że usuwamy wartości z dwóch kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part2 = df[[\"price\", \"it_seller_rating\", \"it_quantity\"]]\n",
    "\n",
    "imputer = KNNImputer(weights=\"uniform\", n_neighbors=5)\n",
    "\n",
    "#ograniczmy liczbę rekordów\n",
    "df_part2 = df_part2[:floor(len(df_part2)*0.1)]\n",
    "rmse1 = [None for i in range(10)]\n",
    "rmse2 = [None for i in range(10)]\n",
    "\n",
    "\n",
    "type(df_part2)\n",
    "len(df_part2)\n",
    "for i in range(10):\n",
    "    df_part2copy = df_part2.copy()\n",
    "    toberemoved = random.sample(range(0, len(df_part2)), floor(len(df_part2)*0.1))\n",
    "    toberemoved2 = random.sample(range(0, len(df_part2)), floor(len(df_part2)*0.1))\n",
    "    df_part2copy.iloc[toberemoved, 1] = None\n",
    "    df_part2copy.iloc[toberemoved, 2] = None\n",
    "\n",
    "    \n",
    "    df_part2copy = pd.DataFrame(imputer.fit_transform(df_part2copy), columns=[\"price\", \"it_seller_rating\", \"it_quantity\"])\n",
    "\n",
    "    \n",
    "    rmse1[i] = np.sqrt(mean_squared_error(df_part2[\"it_seller_rating\"], df_part2copy[\"it_seller_rating\"]))\n",
    "    rmse2[i] = np.sqrt(mean_squared_error(df_part2[\"it_quantity\"], df_part2copy[\"it_quantity\"]))\n",
    "\n",
    "    print(\"RMSE kolumny it_seller_rating: {:.2f}\".format(rmse1[i]))\n",
    "    print(\"RMSE kolumny it_quantity: {:.2f}\".format(rmse2[i]))\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(\"Odchylenie standardowe miary RMSE kolumny it_seller_rating w 10 próbach wynosi: {:.2f}\".format(np.std(rmse1)))\n",
    "#print(\"Odchylenie standardowe miary RMSE kolumny it_quantity w 10 próbach wynosi: {:.2f}\".format(np.std(rmse2)))\n",
    "\n",
    "d = {\"RMSE kolumny it_seller_rating\": rmse1, \"Rmse kolumny it_quantity\":rmse2 }\n",
    "results = pd.DataFrame(d)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "p = plt.plot([i for i in range(1, 11)], rmse, [i for i in range(1, 11)], rmse1)\n",
    "plt.ylim([0, 13000])\n",
    "plt.legend([\"Usunięcie tylko it_seller_rating\", \"Usunięcie obu kolumn\"])\n",
    "plt.title(\"RMSE dla kolumny it_seller_rating\")\n",
    "plt.xlabel(\"numer iteracji\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "sns.boxplot(data=results)\n",
    "plt.ylim([0,13000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-exposure",
   "metadata": {},
   "source": [
    "Z powyższych wykresów widać, że jeśli usuniemy dane z więcej niż jednej kolumny, algorytm imputacji działa gorzej. Może to być spowodowane, że w pierwszym przypadku znajduje \"lepszych\", tzn. bardziej powiązanych sąsiadów. \n",
    "\n",
    "Innym czynnikiem, który może mieć wpływ na wynik, jest użyta metryka. W tym przypadku używam domyślnej metryki euklidesowej."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
