{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-township",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faces.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = faces.data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-prime",
   "metadata": {},
   "source": [
    "## Wybrane zdjęcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_2_img(data, i=None):\n",
    "    plt.gray()\n",
    "    if i is not None:\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.imshow(data[i].reshape(64,64), interpolation='nearest', vmin=0, vmax=1)\n",
    "    else:\n",
    "        plt.imshow(data.reshape(64,64), interpolation='nearest', vmin=0, vmax=1)\n",
    "\n",
    "arr_2_img(data, 0)\n",
    "arr_2_img(data, 1)\n",
    "arr_2_img(data, 2)\n",
    "arr_2_img(data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-wallet",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(data)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-humidity",
   "metadata": {},
   "source": [
    "Przy około 100 komponentach widać załamanie krzywej na wykresie, to znaczy, że powinniśmy dobrać taki parametr `n_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_100 = PCA(n_components=100).fit(data)\n",
    "data_transformed = pca_100.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression = data.shape[1] / data_transformed.shape[1]\n",
    "print('Stopien kompresji = ' + str(round(compression, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived = pca_100.inverse_transform(data_transformed)\n",
    "retrived.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-logistics",
   "metadata": {},
   "source": [
    "## Porównanie oryginalnych zdjęć z odzyskanymi poprzez odwrotną transformację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, name):\n",
    "    arr_2_img(dataset, 0)\n",
    "    arr_2_img(dataset, 1)\n",
    "    arr_2_img(dataset, 2)\n",
    "    arr_2_img(dataset, 3)\n",
    "    plt.suptitle(name)\n",
    "    plt.show()\n",
    "\n",
    "show_samples(retrived, \"Retrived\")\n",
    "show_samples(data, \"Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('RMSE for '+ str(i) + ' photo ' + str(mean_squared_error(data[i], retrived[i], squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE for full dataset =  ' + str(mean_squared_error(data, retrived, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-madagascar",
   "metadata": {},
   "source": [
    "# Przekształcenie oryginalnych zdjęć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-synthesis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rotate_90_add_sym(one_d_arr):\n",
    "    return one_d_arr.reshape(64,64).T.reshape(4096)\n",
    "\n",
    "def bright(one_d_arr, factor):\n",
    "    return one_d_arr * factor\n",
    "\n",
    "def flip_ud(one_d_arr):\n",
    "    return np.flipud(one_d_arr.reshape(64,64)).reshape(4096)\n",
    "\n",
    "\n",
    "arr_2_img(bright(data[0], 2))\n",
    "plt.show()\n",
    "arr_2_img(bright(data[0], 0.5))\n",
    "plt.show()\n",
    "arr_2_img(rotate_90_add_sym(data[0]))\n",
    "plt.show()\n",
    "arr_2_img(flip_ud(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_data = np.array(list(map(rotate_90_add_sym,data)))\n",
    "fliped_data = np.array(list(map(flip_ud,data)))\n",
    "bright_data = np.array(list(map(lambda x: bright(x, 2),data)))\n",
    "dark_data = np.array(list(map(lambda x: bright(x, 0.5),data)))\n",
    "datas = {'rotate':rotated_data, 'flip':fliped_data, 'bright':bright_data, 'dark':dark_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_retrived = {}\n",
    "for name,item in datas.items():\n",
    "    datas_retrived[name] = pca_100.inverse_transform(pca_100.transform(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-hostel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name, item in datas_retrived.items():\n",
    "    show_samples(item, name)\n",
    "    show_samples(datas[name], 'original '+name)\n",
    "    print('RMSE for \"' + name + '\" dataset =  ' + \n",
    "          str(mean_squared_error(datas[name], datas_retrived[name], squared=False)))\n",
    "\n",
    "show_samples(data, \"Original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-character",
   "metadata": {},
   "source": [
    "RMSE dla danych obróconych lub odbitych symetrycznie jest największe, wizualnie zdjęcia po odwrotnej transformacji też nie przypominają tych przed PCA. Co ciekawe, jasne zdjęcia też mają duże RMSE, ale spowodowane jest to wzrostem bezwzględnych wartości poszczególnych pikseli. Z tego samego powodu RMSE dla przyciemnionych zdjęć jest mniejsze niż dla oryginalnego zbioru. Aby móc porówać RMSE możemy je podzielić przez średnią wartość pikseli, wtedy powinniśmy otrzymać porównywalne wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, item in datas_retrived.items():\n",
    "    print('RMSE adjusted for brightness for \"' + name + '\" dataset =  ' + \n",
    "          str(mean_squared_error(datas[name], datas_retrived[name], squared=False) / np.mean(datas[name])))\n",
    "    \n",
    "print('RMSE adjusted for brightness for \"original\" dataset =  ' + \n",
    "          str(mean_squared_error(data, retrived, squared=False) / np.mean(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-portugal",
   "metadata": {},
   "source": [
    "Teraz widać, że przeskalowane względem średniej jasności RMSE jest najmniejsze dla oryginalnych obrazów, a dla obróconych jest zdecydowananie większe.\n",
    "\n",
    "## Do czego może służyć PCA?\n",
    "\n",
    "Ten algorytm może służyć do wykrywania niestandardowej orientacji zdjęcia i triggerować automatyczny obrót. Takie narzędzie mogłoby znaleźć zastosowanie w aparatach fotograficznych lub aplikacjach do przeglądania zdjęć. Orientacja wszystkich portretów mogłyby być automatycznie ustawiana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-muscle",
   "metadata": {},
   "source": [
    "## PCA losowego szumu\n",
    "\n",
    "Byłem ciekawy jak wygląda PCA dla losowego obrazka. Poniżej widać że algorytm zapamiętał średnie wysy twarzy i dopasował do nich szum. Widać też zarys okularów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "rand = np.random.rand(4096).reshape(1,-1)\n",
    "arr_2_img(rand)\n",
    "plt.show()\n",
    "rand_inv = pca_100.inverse_transform(pca_100.transform(rand))\n",
    "arr_2_img(rand_inv)\n",
    "plt.show()\n",
    "print('RMSE adjusted for brightness for \"random\" photo =  ' + \n",
    "          str(mean_squared_error(rand, rand_inv, squared=False) / np.mean(rand)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
