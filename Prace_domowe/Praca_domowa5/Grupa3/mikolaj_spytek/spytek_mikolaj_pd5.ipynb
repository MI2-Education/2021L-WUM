{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moral-dating",
   "metadata": {},
   "source": [
    "# Praca domowa 5\n",
    "**Mikołaj Spytek**\n",
    "\n",
    "Celem tej pracy domowej jest zastosowanie co najmniej dwóch metryk do wyboru odpowiedniej liczby klastrów, dwóch metod klastrowania i wizualizacja wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clustering.csv\", names=[\"x\",\"y\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dzięki temu, że dane są w R^2, możemy po prostu na nie popatrzeć\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-battlefield",
   "metadata": {},
   "source": [
    "## Metoda 1 - KMeans\n",
    "Jako pierwszej metody klasteryzacji użyję KMeans. Aby dobrać odpowiednią liczbę klastrów sprawdzę metrykę Silhouette score oraz metodę, która na laboratorium nazwana była metodą łokcia - policzymy sumę kwadratów odległości od centrum klastra dla każdego klastra i optycznie na wykresie ocenimy gdzie występuje największe przegięcie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-tiger",
   "metadata": {},
   "source": [
    "Na początku spróbuję dobrać optymalną liczbę klastrów za pomocą silhouette score. Miara ta mierzy średnią odległość próbek w klastrze, oraz średnią odległość między klastrami i wyznacza stosunek różnicy tych dwóch liczb do maksimum z nich. Taka definicja pozwala wnioskować, że najlepsze klastrowanie jest dla tej liczby klastrów, dla której miara ta jest największa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scores = []\n",
    "\n",
    "for number_of_clusters in range(2, 16):\n",
    "    kmeans = KMeans(n_clusters=number_of_clusters)\n",
    "    kmeans.fit(df)\n",
    "    clusters = kmeans.predict(df)\n",
    "    scores.append(silhouette_score(df, clusters))\n",
    "    \n",
    "x = [i for i in range(2,16)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x, scores)\n",
    "plt.xlabel(\"number of clusters\")\n",
    "plt.ylabel(\"silhouette score\")\n",
    "plt.title(\"Silhouette score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-mobility",
   "metadata": {},
   "source": [
    "Łatwo zauważyć, że metoda silhouette wskazuje, że optymalną liczbą klastrów będzie 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-ratio",
   "metadata": {},
   "source": [
    "Postanowiłem też sprawdzić, czy taki sam wynik można otrzymać metodą łokcia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for num_clusters in range(2, 12):\n",
    "    kmeans = KMeans(n_clusters = num_clusters)\n",
    "    kmeans.fit(df)\n",
    "    score = kmeans.score(df) * (-1)\n",
    "    scores.append(score)\n",
    "    \n",
    "x = [i for i in range(2,12)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x, scores)\n",
    "plt.xlabel(\"number of clusters\")\n",
    "plt.ylabel(\"within cluster sum of squares\")\n",
    "plt.title(\"WCSS score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-pension",
   "metadata": {},
   "source": [
    "Tutaj nie mamy już wyraźnego wskazania. Jeżeli już miałbym się zdecydować, to powiedziałbym, że największe przegięcie występuje w punkcie 3, a więc optymalną liczbą klastrów byłoby 4, co zupełnie nie pokrywa się ze wskazaniem poprzedniej metody.\n",
    "\n",
    "Ponieważ mamy ten komfort, że dane są w $\\mathbb{R}^2$, możemy je zwizualizować i zobaczyć jaka liczba klastrów jest lepsza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=8)\n",
    "kmeans1.fit(df)\n",
    "kmeanslabels1 = kmeans1.predict(df)\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=4)\n",
    "kmeans2.fit(df)\n",
    "kmeanslabels2 = kmeans2.predict(df)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=kmeanslabels1, ax = ax1,  palette=sns.color_palette('muted', n_colors=8))\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=kmeanslabels2, ax = ax2,  palette=sns.color_palette('muted', n_colors=4))\n",
    "ax1.set_title(\"Klastrowanie na 8 klastrów\")\n",
    "ax2.set_title(\"Klastrowanie na 4 klastry\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-winter",
   "metadata": {},
   "source": [
    "Z powyższych wykresów widać, że metoda silhouette zadziałała dużo lepiej. Przy podziale na 4 klastry widać, że punkty, które wizualnie są w oddzielnych klastrach, zostały przydzielone do tego samego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-dimension",
   "metadata": {},
   "source": [
    "## Metoda 2 - Agglomerative Clustering\n",
    "\n",
    "Jako drugiej metody klasteryzacji użyję Agglomerative Clustering. Sposób działania tego algorytmu wygląda następująco: każda obserwacja zaczyna w osobnym klastrze, a następnie są one iteracyjnie łączone. Z domyślną metodą łączenia (ward), kryterium połączenia jest takie, aby nowe klastry miały jak najmniejszą wariancję. Proces ten powtarzany jest, aż zostanie tylko żądana liczba klastrów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-municipality",
   "metadata": {},
   "source": [
    "Ponieważ ostatnio ta metryka sprawdziła się bardzo dobrze zastosujmy silhouette score jeszcze raz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scores = []\n",
    "\n",
    "for number_of_clusters in range(2, 16):\n",
    "    clusterer = AgglomerativeClustering(n_clusters=number_of_clusters)\n",
    "    clusterer.fit(df)\n",
    "    clusters = clusterer.labels_\n",
    "    scores.append(silhouette_score(df, clusters))\n",
    "    \n",
    "x = [i for i in range(2,16)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x, scores)\n",
    "plt.xlabel(\"number of clusters\")\n",
    "plt.ylabel(\"silhouette score\")\n",
    "plt.title(\"Silhouette score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-preliminary",
   "metadata": {},
   "source": [
    "Jak widać, również w przypadku klastrowania hierarchicznego, optymalną liczbą klastrów według tej metody jest 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-davis",
   "metadata": {},
   "source": [
    "Jako drugiej metody użyłem miary Daviesa-Bouldina. Zdefiniowana jest ona jako średnia podobieństwa każdego klastra, z klastrem najbardziej do niego podobnym, a podobieństwo jest określone jako iloraz średniej odległości wewnątrz klastra do średniej odległości pomiędzy klastrami. Widzimy, że przy takiej definicji interesować nas będą niskie wartości tej miary - chcemy, aby klastry były od siebie jak najbardziej różne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "scores = []\n",
    "\n",
    "for number_of_clusters in range(2, 16):\n",
    "    clusterer = AgglomerativeClustering(n_clusters=number_of_clusters)\n",
    "    clusterer.fit(df)\n",
    "    clusters = clusterer.labels_\n",
    "    scores.append(davies_bouldin_score(df, clusters))\n",
    "    \n",
    "x = [i for i in range(2,16)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x, scores)\n",
    "plt.xlabel(\"number of clusters\")\n",
    "plt.ylabel(\"Davies-Bouldin score\")\n",
    "plt.title(\"Davoes-Bouldin score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-calgary",
   "metadata": {},
   "source": [
    "Miara ta wskazuje nam, że optymalną liczbą klastrów będzie 6. Sprawdźmy i porównajmy jak wyglądają klastrowania z 6-cioma i 8-mioma klastrami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg1 = AgglomerativeClustering(n_clusters=8)\n",
    "agg1.fit(df)\n",
    "agglabels1 = agg1.labels_\n",
    "\n",
    "agg2 = AgglomerativeClustering(n_clusters=6)\n",
    "agg2.fit(df)\n",
    "agglabels2 = agg2.labels_\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=agglabels1, ax = ax1,  palette=sns.color_palette('muted', n_colors=8))\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=agglabels2, ax = ax2,  palette=sns.color_palette('muted', n_colors=6))\n",
    "ax1.set_title(\"Klastrowanie na 8 klastrów\")\n",
    "ax2.set_title(\"Klastrowanie na 6 klastrów\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-danish",
   "metadata": {},
   "source": [
    "Widać wyraźnie, że 8 klastrów jest lepszą liczbą. Na przykład na prawym rysunku niebieski klaster powinien być rozbity na dwie części, tak samo jak zielony.\n",
    "\n",
    "Można jeszcze sprawdzić, w jakich punktach optymalne klastrowania tymi dwoma metodami się różnią:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=kmeanslabels1, ax = ax1,  palette=sns.color_palette('muted', n_colors=8))\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=agglabels1, ax = ax2,  palette=sns.color_palette('muted', n_colors=8))\n",
    "ax1.set_title(\"Klastrowanie za pomocą KMeans\")\n",
    "ax2.set_title(\"Klastrowanie za pomocą AgglomerativeClustering\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-pipeline",
   "metadata": {},
   "source": [
    "Widać, że klastry różnią się tylko obserwacjami, które są na granicy, tak, że nawet człowiekowi ciężko byłoby określić, do którego klastra powinny należeć."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
