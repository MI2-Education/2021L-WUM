{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex as dx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalex_df = dx.datasets.load_apartments()\n",
    "dalex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalex_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-retreat",
   "metadata": {},
   "source": [
    "Jako drugi zbiór danych wziąłem zbiór dotyczący mieszkań w Bostonie z Lab1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dict = load_boston()\n",
    "boston_df = pd.DataFrame(boston_dict['data'], columns=boston_dict['feature_names'])\n",
    "boston_df['MEDV'] = boston_dict['target']\n",
    "\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dalex_df['district'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-tourism",
   "metadata": {},
   "source": [
    "Ponieważ jest tylko 10 kategorii w ramce DALEX, użyjemy one-hot encodingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalex_df_enc = pd.concat([\n",
    "    pd.get_dummies(dalex_df.district, prefix='District'),\n",
    "    dalex_df], axis=1).drop(['district'], axis=1)\n",
    "\n",
    "# zmieńmy jeszcze kolejność kolumn na bardziej intuicyjną\n",
    "\n",
    "cols = dalex_df_enc.columns.tolist()\n",
    "cols = cols[-4:] + cols[:-4]\n",
    "\n",
    "dalex_df_enc = dalex_df_enc[cols]\n",
    "dalex_df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dalex = dalex_df_enc.drop('m2_price', axis=1)\n",
    "Y_dalex = dalex_df_enc.m2_price\n",
    "\n",
    "X_boston = boston_df.drop(['MEDV'], axis=1)\n",
    "Y_boston = boston_df['MEDV']\n",
    "\n",
    "X_train_dalex, X_test_dalex, y_train_dalex, y_test_dalex = train_test_split(\n",
    "    X_dalex, Y_dalex, test_size = 0.33, random_state = 34)\n",
    "\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(\n",
    "    X_boston, Y_boston, test_size = 0.33, random_state = 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-stewart",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVR()\n",
    "svm.fit(X_train_dalex, y_train_dalex)\n",
    "y_hat_dalex = svm.predict(X_test_dalex)\n",
    "print(\"Dalex\")\n",
    "print(\"Wynik R2: \" + str(r2_score(y_test_dalex, y_hat_dalex)))\n",
    "print(\"Miara RMSE: \" + str(mean_squared_error(y_test_dalex, y_hat_dalex, squared = False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przeskalujmy nasze dane i ponownie zbudujmy model\n",
    "scaler = MinMaxScaler()\n",
    "dalex_df_enc[['construction_year', 'surface', 'floor', 'no_rooms']] = scaler.fit_transform(dalex_df_enc[[\n",
    "    'construction_year', 'surface', 'floor', 'no_rooms']])\n",
    "\n",
    "X_dalex = dalex_df_enc.drop('m2_price', axis=1)\n",
    "Y_dalex = dalex_df_enc.m2_price\n",
    "\n",
    "X_train_dalex, X_test_dalex, y_train_dalex, y_test_dalex = train_test_split(\n",
    "    X_dalex, Y_dalex, test_size = 0.33, random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVR()\n",
    "svm.fit(X_train_dalex, y_train_dalex)\n",
    "y_hat_dalex = svm.predict(X_test_dalex)\n",
    "print(\"Dalex po przeskalowaniu\")\n",
    "print(\"Wynik R2: \" + str(r2_score(y_test_dalex, y_hat_dalex)))\n",
    "print(\"Miara RMSE: \" + str(mean_squared_error(y_test_dalex, y_hat_dalex, squared = False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-verification",
   "metadata": {},
   "source": [
    "Widzimy, że po przeskalowaniu wyniki modelu uległy poprawieniu. Ten sam eksperyment przeprowadźmy dla datasetu bostońskiego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_boston = SVR()\n",
    "svm_boston.fit(X_train_boston, y_train_boston)\n",
    "y_hat_boston = svm_boston.predict(X_test_boston)\n",
    "print(\"Boston\")\n",
    "print(\"Wynik R2: \" + str(r2_score(y_test_boston, y_hat_boston)))\n",
    "print(\"Miara RMSE: \" + str(mean_squared_error(y_test_boston, y_hat_boston, squared = False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "boston_df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']] = scaler.fit_transform(boston_df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']])\n",
    "\n",
    "X_boston = boston_df.drop('MEDV', axis=1)\n",
    "Y_boston = boston_df.MEDV\n",
    "\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(\n",
    "    X_boston, Y_boston, test_size = 0.33, random_state = 34)\n",
    "\n",
    "svm = SVR()\n",
    "svm.fit(X_train_boston, y_train_boston)\n",
    "y_hat_boston = svm.predict(X_test_boston)\n",
    "print(\"Boston po przeskalowaniu\")\n",
    "print(\"Wynik R2: \" + str(r2_score(y_test_boston, y_hat_boston)))\n",
    "print(\"Miara RMSE: \" + str(mean_squared_error(y_test_boston, y_hat_boston, squared = False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-holiday",
   "metadata": {},
   "source": [
    "Wniosek: Skalowanie danych przynosi dobre efekty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-architect",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    C = np.arange(start = 0.1, stop = 10000, step = 0.05),\n",
    "    gamma = ['scale', 'auto'],\n",
    "    degree = np.arange(1, 80, 1))\n",
    "\n",
    "svm_rand_dalex = RandomizedSearchCV(svm_boston, parameters, cv=3, n_iter=200)\n",
    "\n",
    "svm_rand_dalex.fit(X_train_dalex, y_train_dalex)\n",
    "print(\"Najlepsze parametry: \" + str(svm_rand_dalex.best_params_))\n",
    "\n",
    "best_estimator = svm_rand_dalex.best_estimator_\n",
    "print(\"Wynik R2: \" + str(r2_score(y_test_dalex, best_estimator.predict(X_test_dalex))))\n",
    "print(f'RMSE: {mean_squared_error(y_test_dalex, best_estimator.predict(X_test_dalex), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rand_boston = RandomizedSearchCV(svm_boston, parameters, cv=3, n_iter=200)\n",
    "\n",
    "svm_rand_boston.fit(X_train_boston, y_train_boston)\n",
    "print(\"Najlepsze parametry: \" + str(svm_rand_boston.best_params_))\n",
    "\n",
    "best_estimator = svm_rand_boston.best_estimator_\n",
    "print(\"Wynik R2: \" + str(r2_score(y_test_boston, best_estimator.predict(X_test_boston))))\n",
    "print(f'RMSE: {mean_squared_error(y_test_boston, best_estimator.predict(X_test_boston), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-award",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
